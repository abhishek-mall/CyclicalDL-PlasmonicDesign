import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import torchvision
import torchvision.utils as vutils
from torch.distributions import normal
import glob
import cv2
import numpy as np
from PIL import Image
import os
from random import shuffle
import pandas as pd
from create_dataset_CGAN_new import read_data
from model_CGAN import Generator, Discriminator
import torchvision.transforms.functional as TF

dir_ = r"F:\AE+GAN\Halfwaveplate\CGAN\Revision 1\Data"
#Parameters
#number of sample in batches during training
batchSize = 64

#number of Training epochs
num_epochs = 5000
#learning rate of the optimizers
lr1 = 2e-4
lr2 = 2e-4
# Beta1 hyperparam for Adam optimizers
beta1= 0.5


#The device to run on
use_cuda = 1
device = torch.device("cuda" if use_cuda else "cpu")

#UTILITY FUNCTIONS
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
  
#Helper function
def load_checkpoint(filepath):
    checkpoint = torch.load(filepath)
    model = checkpoint['model']
    model.load_state_dict(checkpoint['state_dict'])
    for parameter in model.parameters():
        parameter.requires_grad = True
    model.eval()
    return model

def EPE(input_flow, target_flow, sparse=False, mean=True):
    EPE_map = torch.norm(target_flow-input_flow,2,1)
    batch_size = EPE_map.size(0)
    if sparse:
        # invalid flow is defined with both flow coordinates to be exactly 0
        mask = (target_flow[:,0] == 0) & (target_flow[:,1] == 0)

        EPE_map = EPE_map[~mask]
    if mean:
        return EPE_map.mean()
    else:
        return EPE_map.sum()/batch_size


#INITIALISING MODELS

discriminator = Discriminator().to(device)
generator = Generator().to(device)


#Weights intialization for G and D
discriminator.apply(weights_init)
generator.apply(weights_init)

#Loss
adversarial_loss = torch.nn.BCELoss()

# Setup Adam optimizers for both Generator  and Discriminiator
optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr1, betas=(beta1, 0.999))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr2, betas=(beta1, 0.999))


#Lists to keep track of training progress
img_list = []
G_losses = []
D_losses = []
iters = 0

#Here we validate our network
dataTrain, dataVal = read_data(dir_)


def validate(epoch, Valdataset):
     try:
        for i, (imgs, spctr) in enumerate(Valdataset):
            print("----------------------------THIS IS MINI BATCH NUMBER---------------------", i+1)
            # spctr_tensor = torch.zeros(spctr.shape[0], spctr.shape[1], spctr.shape[1])
            batch_size = imgs.shape[0]
            valid = torch.ones(batch_size, 1).to(device) #Real label for Imagess
            fake = torch.zeros(batch_size, 1).to(device) #Fake label for Images.
            real_imgs = imgs.to(device)
            real_imgs = real_imgs.float()
            spctr = spctr.to(device)
            spctr = spctr.float()
            spctr = spctr.view(spctr.shape[0], spctr.shape[1], 1, 1)
            z = torch.cat((torch.rand(batch_size, 923, 1, 1).to(device), spctr), 1)
            gen_imgs = generator(z)   
            #Update D network: maximize log(D(x)) + log(1 - D(G(z)))
            discriminator.zero_grad()
            optimizer_D.zero_grad()
            validity_real = discriminator(real_imgs, spctr)    #Forward pass for all real batches
            d_real_loss = adversarial_loss(validity_real, valid)  #Discriminator loss on real batches
            validity_fake = discriminator(gen_imgs.detach(), spctr)  #Passing fake batches to Discriminator
            d_fake_loss = adversarial_loss(validity_fake, fake)       #Discriminator loss on fake batches
            d_loss = (d_real_loss + d_fake_loss)  #All the gradients from all real and fake

            #Update G network: maximize log(D(G(z)))
            generator.zero_grad()
            optimizer_G.zero_grad()
            #Creating a batch of latent vector
            #Images generated by generator
            validity = discriminator(gen_imgs, spctr)  #Forward pass for all fake batches.
            g_loss = adversarial_loss(validity, valid)   #Loss of the Generator ?????
            
            print("Generator loss", g_loss.item())
            print("Discriminator loss", d_loss.item())
            
             # Save Losses for plotting later
            G_losses.append(g_loss.item())
            D_losses.append(d_loss.item())
            # spctr_image = create_image(spctr)
            gen_imgs = gen_imgs.cpu()
            real_imgs = real_imgs.cpu()
            spctr = spctr.cpu()

            if((epoch+1)%5 == 0):
                
                for im in range(0, real_imgs.shape[0]):
                    name_gen = dir_ +"/"+ "fakeVal/" + str(epoch) + "_" + str(im) + ".png"
                    gen_imgs_save = gen_imgs[im].view(gen_imgs[im].shape[1], gen_imgs[im].shape[2], gen_imgs[im].shape[0])
                    gen_imgs_save = gen_imgs_save.cpu().detach().numpy()
                    cv2.imwrite(name_gen, gen_imgs_save)

                    name_real = dir_ +"/"+ "realVal/" + str(epoch) + "_" + str(im) + ".png"
                    real_imgs_save = real_imgs[im].view(real_imgs[im].shape[1], real_imgs[im].shape[2], real_imgs[im].shape[0])
                    real_imgs_save = real_imgs_save.cpu().detach().numpy()
                    cv2.imwrite( name_real, real_imgs_save)

                    name_spctr = dir_ +"/"+ "spctrVal/" + str(epoch) + "_" + str(im) + ".csv"
                    spctr_save = spctr[im].view(spctr[im].shape[0])
                    np.savetxt(name_spctr, spctr_save, delimiter=",")  
            
     except TypeError:
         print("error")


print("Starting Training Loop...")
#For each epoch

GLossFile = open("GLoss.txt", "w")
DLossFile = open("DLoss.txt", "w")

for epoch in range (num_epochs):
    
    Traindataset = torch.utils.data.DataLoader(dataTrain, batchSize, shuffle = True)
    Valdataset = torch.utils.data.DataLoader(dataVal, batchSize, shuffle = True)
    avg_gen_loss , avg_dis_loss = 0, 0

    print("----------------------------THIS EPOCH IS-------------------------", epoch+1)
    
    if(epoch and (epoch+1)%5 == 0):
         validate(epoch, Valdataset)

         
    if(epoch and epoch %5 ==0):
        checkpoint_en = {'model': Generator() ,'state_dict': generator.state_dict(), 'optimizer' : optimizer_G.state_dict()}
        torch.save(checkpoint_en, dir_+"/"+'Weights/'+'gen'+str(epoch)+'.pth')
    
        checkpoint_en = {'model': Discriminator() ,'state_dict': discriminator.state_dict(), 'optimizer' : optimizer_D.state_dict()}
        torch.save(checkpoint_en, dir_+"/"+'Weights/'+'dis'+str(epoch)+'.pth')
    # For each batch in the dataloader
    # try:
    for i, (imgs, spctr) in enumerate(Traindataset):
        #print("----------------------------THIS IS MINI BATCH NUMBER---------------------", i+1)
        
        batch_size = imgs.shape[0]
        valid = torch.ones(batch_size, 1).to(device) #Real label for Images
        fake = torch.zeros(batch_size, 1).to(device) #Fake label for Images.
        real_imgs = imgs.to(device)
        real_imgs = real_imgs.float()
        

        spctr = spctr.to(device)
        spctr = spctr.float()
        spctr = spctr.view(spctr.shape[0], spctr.shape[1], 1, 1)
        z = torch.cat((torch.rand(batch_size, 923, 1, 1).to(device), spctr), 1)
        gen_imgs = generator(z)   
        #Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        discriminator.zero_grad()
        optimizer_D.zero_grad()
        validity_real = discriminator(real_imgs, spctr)    #Forward pass for all real batches
        d_real_loss = adversarial_loss(validity_real, valid)  #Discriminator loss on real batches
        validity_fake = discriminator(gen_imgs.detach(), spctr)  #Passing fake batches to Discriminator
        d_fake_loss = adversarial_loss(validity_fake, fake)       #Discriminator loss on fake batches
        d_loss = (d_real_loss + d_fake_loss)*0.5  #All the gradients from all real and fakes.
        d_loss.backward()
        optimizer_D.step()


        #Update G network: maximize log(D(G(z)))
        generator.zero_grad()
        optimizer_G.zero_grad()
        #Creating a batch of latent vector
        
        # print(spctr.shape)
        #Images generated by generator
        validity = discriminator(gen_imgs, spctr)  #Forward pass for all fake batches.
        g_loss = adversarial_loss(validity, valid) #Loss of the Generator ?????
        g_loss.backward()                            #Calculating gradient of Generator
        optimizer_G.step()
        
        avg_gen_loss += g_loss.detach()
        avg_dis_loss += d_loss.detach()
          
     
        #Output training stats
       # print("Generator loss", g_loss.item())
        #print("Discriminator loss", d_loss.item())
        # Save Losses for plotting later
        G_losses.append(g_loss.item())
        D_losses.append(d_loss.item())
        

        #Saving stuff
        gen_imgs = gen_imgs.cpu()
        real_imgs = real_imgs.cpu()
        spctr = spctr.cpu()
        
        
        
        if((epoch+1)%5 == 0):
            
            for im in range(0, real_imgs.shape[0]):
                name_gen = dir_ +"/"+ "faketrain/" + str(epoch) + "_" + str(im) + ".png"
                gen_imgs_save = gen_imgs[im].view(gen_imgs[im].shape[1], gen_imgs[im].shape[2], gen_imgs[im].shape[0])
                gen_imgs_save = gen_imgs_save.cpu().detach().numpy()
                cv2.imwrite(name_gen, gen_imgs_save)
    
                name_real = dir_ +"/"+ "realtrain/" + str(epoch) + "_" + str(im) + ".png"
                real_imgs_save = real_imgs[im].view(real_imgs[im].shape[1], real_imgs[im].shape[2], real_imgs[im].shape[0])
                real_imgs_save = real_imgs_save.cpu().detach().numpy()
                cv2.imwrite( name_real, real_imgs_save)
    
                name_spctr = dir_ +"/"+ "spctrtrain/" + str(epoch) + "_" + str(im) + ".csv"
                spctr_save = spctr[im].view(spctr[im].shape[0])
                np.savetxt(name_spctr, spctr_save, delimiter=",")  
            
    avg_gen_loss = avg_gen_loss/(i+1)
    avg_dis_loss = avg_dis_loss/(i+1)
    print("Average generator loss", avg_gen_loss.item())
    print("Average discriminator loss", avg_dis_loss.item())
    GLossFile.write(str(avg_gen_loss.item()))
    GLossFile.write("\n")
    DLossFile.write(str(avg_dis_loss.item()))
    DLossFile.write("\n")
